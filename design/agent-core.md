# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¸­æ ¸ãƒ­ã‚¸ãƒƒã‚¯è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆTiny Agentï¼‰

## æ¦‚è¦

ygentsãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¸­æ ¸ãƒ­ã‚¸ãƒƒã‚¯ã¯ã€å˜ç´”ãªcompletionãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚Šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚è¤‡é›‘ãªã‚¿ã‚¹ã‚¯è¨ˆç”»ãƒ»å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ã§ã¯ãªãã€ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ™ãƒ¼ã‚¹ã®å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã‚’æ¡ç”¨ã—ã¾ã™ã€‚TDDæ–¹å¼ã§å®Ÿè£…ã—ã€LiteLLMã¨FastMCPã‚’ç›´æ¥åˆ©ç”¨ã™ã‚‹ç°¡ç´ åŒ–ã•ã‚ŒãŸè¨­è¨ˆã§ã™ã€‚

## è¨­è¨ˆæ–¹é‡

### Tiny Agent - å˜ç´”åŒ–ã•ã‚ŒãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

- **è¤‡é›‘ãªè¨ˆç”»ã‚·ã‚¹ãƒ†ãƒ ä¸è¦**: ãƒ¦ãƒ¼ã‚¶ãƒ¼å•é¡Œè§£æ±ºã¾ã§å˜ç´”ãªcompletionãƒ«ãƒ¼ãƒ—
- **ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ™ãƒ¼ã‚¹**: ä¼šè©±å±¥æ­´ã‚’messagesé…åˆ—ã§ç®¡ç†
- **ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—çµ±åˆ**: completionçµæœã«åŸºã¥ãè‡ªå‹•ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
- **å•é¡Œè§£æ±ºåˆ¤å®š**: å˜ç´”ãªçµ‚äº†æ¡ä»¶ãƒã‚§ãƒƒã‚¯

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆç°¡ç´ åŒ–ï¼‰

```
src/ygents/agent/
â”œâ”€â”€ __init__.py         # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åˆæœŸåŒ–
â”œâ”€â”€ core.py             # Agentä¸­æ ¸ã‚¯ãƒ©ã‚¹ï¼ˆå˜ç´”åŒ–ï¼‰
â””â”€â”€ exceptions.py       # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå°‚ç”¨ä¾‹å¤–
```

**å‰Šé™¤ã•ã‚Œã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«**:
- `planner.py` - è¤‡é›‘ãªã‚¿ã‚¹ã‚¯è¨ˆç”»ã¯ä¸è¦
- `executor.py` - å˜ç´”ãªãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã§ååˆ†

## ã‚³ã‚¢ã‚¯ãƒ©ã‚¹è¨­è¨ˆ

### Agent ã‚¯ãƒ©ã‚¹

```python
# src/ygents/agent/core.py
import asyncio
from typing import Optional, Dict, Any, List, AsyncGenerator
import fastmcp
import litellm
from ..config.models import YgentsConfig
from .exceptions import AgentError, AgentConnectionError

class Agent:
    """Tiny Agent - å˜ç´”ãªcompletionãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚‹å•é¡Œè§£æ±ºã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    
    è¤‡é›‘ãªã‚¿ã‚¹ã‚¯è¨ˆç”»ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ãªãã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ™ãƒ¼ã‚¹ã®å˜ç´”ãª
    å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚
    """
    
    def __init__(self, config: YgentsConfig):
        self.config = config
        self._mcp_client: Optional[fastmcp.Client] = None
        self._mcp_client_connected = False
        self.messages: List[Dict[str, Any]] = []  # ä¼šè©±å±¥æ­´
    
    async def __aenter__(self) -> "Agent":
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–‹å§‹æ™‚ã«MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ãƒ»æ¥ç¶š"""
        if self.config.mcp_servers:
            fastmcp_config = {"mcpServers": self.config.mcp_servers}
            self._mcp_client = fastmcp.Client(fastmcp_config)
            await self._mcp_client.__aenter__()
            self._mcp_client_connected = True
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ‚äº†æ™‚ã«MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆ‡æ–­"""
        if self._mcp_client and self._mcp_client_connected:
            await self._mcp_client.__aexit__(exc_type, exc_val, exc_tb)
            self._mcp_client_connected = False
    
    @property
    def available_tools(self) -> List[Dict[str, Any]]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        if self._mcp_client and self._mcp_client_connected:
            try:
                # åŒæœŸçš„ã«ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’è¿”ã™ãŸã‚ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
                # å®Ÿéš›ã®å–å¾—ã¯éåŒæœŸã§äº‹å‰ã«è¡Œã†
                return getattr(self, '_cached_tools', [])
            except Exception:
                return []
        return []
    
    async def run(self, user_input: str, abort_event=None) -> AsyncGenerator[Any, None]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¯¾ã—ã¦å•é¡Œè§£æ±ºã¾ã§å˜ç´”ãªcompletionãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ
        
        REQUIREMENTS.mdã®ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã«åŸºã¥ãå®Ÿè£…:
        1. user_inputã‚’messagesã«è¿½åŠ 
        2. å•é¡Œè§£æ±ºã¾ã§ completion ãƒ«ãƒ¼ãƒ—ã‚’å›ã™
        3. å¿…è¦ã«å¿œã˜ã¦ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
        4. å•é¡Œè§£æ±ºåˆ¤å®šã§çµ‚äº†
        """
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã«è¿½åŠ 
        self.messages.append({"role": "user", "content": user_input})
        
        while True:
            # å˜ä¸€ã‚¿ãƒ¼ãƒ³ã§ã®ãƒ„ãƒ¼ãƒ«ä»˜ãcompletionå‡¦ç†
            loop_completed = False
            async for item in self.process_single_turn_with_tools(self.messages):
                yield item
                
                # å•é¡Œè§£æ±ºåˆ¤å®šï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰
                if self._is_problem_solved(item):
                    loop_completed = True
                    break
            
            if loop_completed:
                break
                
            # ä¸­æ–­ã‚¤ãƒ™ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯
            if abort_event and abort_event.is_set():
                yield {"type": "status", "content": "å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ"}
                break
    
    async def process_single_turn_with_tools(self, messages: List[Dict[str, Any]]) -> AsyncGenerator[Any, None]:
        """å˜ä¸€ã‚¿ãƒ¼ãƒ³ã§ã®LLM completion + ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
        
        1. LLM completionã‚’å®Ÿè¡Œ
        2. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’yieldï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰
        3. ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚Œã°å®Ÿè¡Œ
        4. ãƒ„ãƒ¼ãƒ«çµæœã‚’messagesã«è¿½åŠ ã—ã¦yield
        """
        try:
            # LiteLLMã§completionå®Ÿè¡Œï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
            response = await litellm.acompletion(
                model=self._get_model_name(),
                messages=messages,
                tools=self._get_tools_schema() if self._mcp_client_connected else None,
                stream=True,
                **self._get_llm_params()
            )
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†
            assistant_message = {"role": "assistant", "content": "", "tool_calls": []}
            
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    assistant_message["content"] += content
                    yield {"type": "content", "content": content}
                
                if chunk.choices[0].delta.tool_calls:
                    for tool_call in chunk.choices[0].delta.tool_calls:
                        assistant_message["tool_calls"].append(tool_call)
            
            # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
            self.messages.append(assistant_message)
            
            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å®Ÿè¡Œ
            if assistant_message.get("tool_calls"):
                async for tool_result in self._execute_tool_calls(assistant_message["tool_calls"]):
                    yield tool_result
                    
        except Exception as e:
            yield {"type": "error", "content": f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"}
            raise AgentError(f"Completion failed: {e}") from e
    
    async def _execute_tool_calls(self, tool_calls: List[Dict[str, Any]]) -> AsyncGenerator[Any, None]:
        """ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å®Ÿè¡Œã—ã€çµæœã‚’yield"""
        for tool_call in tool_calls:
            tool_name = tool_call.get("function", {}).get("name")
            arguments = tool_call.get("function", {}).get("arguments", {})
            
            yield {"type": "tool_input", "tool_name": tool_name, "arguments": arguments}
            
            try:
                if self._mcp_client and self._mcp_client_connected:
                    result = await self._mcp_client.call_tool(tool_name, arguments)
                    
                    # ãƒ„ãƒ¼ãƒ«çµæœã‚’messagesã«è¿½åŠ 
                    self.messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.get("id"),
                        "content": str(result)
                    })
                    
                    yield {"type": "tool_result", "tool_name": tool_name, "result": result}
                else:
                    yield {"type": "tool_error", "content": "MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
                    
            except Exception as e:
                error_msg = f"ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({tool_name}): {e}"
                self.messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.get("id"),
                    "content": error_msg
                })
                yield {"type": "tool_error", "content": error_msg}
    
    def _is_problem_solved(self, item: Dict[str, Any]) -> bool:
        """å•é¡Œè§£æ±ºåˆ¤å®šï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰"""
        # TODO: ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸçµ‚äº†åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
        if item.get("type") == "content":
            content = item.get("content", "").lower()
            # ç°¡å˜ãªçµ‚äº†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
            end_keywords = ["å®Œäº†", "çµ‚äº†", "è§£æ±º", "ã§ãã¾ã—ãŸ", "finished", "done"]
            return any(keyword in content for keyword in end_keywords)
        return False
    
    def _get_model_name(self) -> str:
        """è¨­å®šã‹ã‚‰ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—"""
        provider = self.config.llm.provider
        
        if provider == "openai":
            return self.config.llm.openai.model
        elif provider == "claude":
            return self.config.llm.claude.model
        else:
            raise ValueError(f"Unsupported provider: {provider}")
    
    def _get_llm_params(self) -> Dict[str, Any]:
        """LiteLLMå‘¼ã³å‡ºã—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        params = {
            "temperature": 0.7,
            "max_tokens": 1000,
        }
        
        provider = self.config.llm.provider
        
        if provider == "openai":
            params["api_key"] = self.config.llm.openai.api_key
        elif provider == "claude":
            params["api_key"] = self.config.llm.claude.api_key
        
        return params
    
    def _get_tools_schema(self) -> List[Dict[str, Any]]:
        """MCPãƒ„ãƒ¼ãƒ«ã‚’LiteLLM toolså½¢å¼ã«å¤‰æ›"""
        if not self._mcp_client_connected:
            return []
        
        # å®Ÿè£…: MCPãƒ„ãƒ¼ãƒ«å®šç¾©ã‚’OpenAI toolså½¢å¼ã«å¤‰æ›
        # è©³ç´°ã¯å®Ÿè£…æ™‚ã«èª¿æ•´
        return []
    
    async def _cache_available_tools(self):
        """ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆåˆæœŸåŒ–æ™‚ã«å®Ÿè¡Œï¼‰"""
        if self._mcp_client and self._mcp_client_connected:
            try:
                tools = await self._mcp_client.list_tools()
                self._cached_tools = tools
            except Exception:
                self._cached_tools = []
```

## ä½¿ç”¨ä¾‹

### åŸºæœ¬çš„ãªä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆREQUIREMENTS.mdã®ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã«åŸºã¥ãï¼‰

```python
async def main():
    config = load_config("config.yaml")
    
    async with Agent(config) as agent:
        # åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ä¸€è¦§è¡¨ç¤º
        for tool in agent.available_tools:
            print(f"Tool: {tool.get('name', 'Unknown')}")
        
        while True:
            user_input = await async_prompt(exit_event=exit_event)
            
            # user_inputã‚’å…ƒã«å•é¡Œè§£æ±ºã¾ã§å˜ç´”ãªcompletionãƒ«ãƒ¼ãƒ—
            async for chunk in agent.run(user_input, abort_event=abort_event):
                display_chunk(chunk)

def display_chunk(chunk):
    """ãƒãƒ£ãƒ³ã‚¯è¡¨ç¤ºãƒ˜ãƒ«ãƒ‘ãƒ¼"""
    if chunk.get("type") == "content":
        print(chunk["content"], end="", flush=True)
    elif chunk.get("type") == "tool_input":
        print(f"\nğŸ”§ ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ: {chunk['tool_name']}")
        print(f"   å¼•æ•°: {chunk['arguments']}")
    elif chunk.get("type") == "tool_result":
        print(f"âœ… çµæœ: {chunk['result']}")
    elif chunk.get("type") == "error":
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {chunk['content']}")
```

## ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†

### æ°¸ç¶šæ¥ç¶šã¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´

```python
# Tiny Agentã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†
async def interactive_session():
    config = load_config("config.yaml")
    
    async with Agent(config) as agent:
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–‹å§‹æ™‚ã«ä¸€åº¦ã ã‘MCPã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶š
        await agent._cache_available_tools()  # ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’äº‹å‰ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
        print("åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:")
        for tool in agent.available_tools:
            print(f"  - {tool.get('name', 'Unknown')}")
        
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³
        while True:
            user_input = input("\nè³ªå•: ")
            if user_input.lower() in ['quit', 'exit']:
                break
            
            print("å›ç­”:")
            # å˜ç´”ãªcompletionãƒ«ãƒ¼ãƒ—ã§å•é¡Œè§£æ±º
            async for chunk in agent.run(user_input):
                if chunk.get("type") == "content":
                    print(chunk["content"], end="", flush=True)
                elif chunk.get("type") == "tool_input":
                    print(f"\nğŸ”§ {chunk['tool_name']} ã‚’å®Ÿè¡Œä¸­...")
                elif chunk.get("type") == "tool_result":
                    print(f"âœ… å®Œäº†")
            print()  # æ”¹è¡Œ
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ‚äº†æ™‚ã«å…¨MCPæ¥ç¶šãŒè‡ªå‹•çš„ã«é–‰ã˜ã‚‰ã‚Œã‚‹
```

## ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### ä¾‹å¤–éšå±¤ï¼ˆç°¡ç´ åŒ–ï¼‰

```python
# src/ygents/agent/exceptions.py
class AgentError(Exception):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåŸºåº•ä¾‹å¤–"""
    pass

class AgentConnectionError(AgentError):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ¥ç¶šã‚¨ãƒ©ãƒ¼"""
    pass
```

### ã‚¨ãƒ©ãƒ¼å‡¦ç†æˆ¦ç•¥ï¼ˆç°¡ç´ åŒ–ï¼‰

Tiny Agentã§ã¯ã€ã‚¨ãƒ©ãƒ¼ã¯`run`ãƒ¡ã‚½ãƒƒãƒ‰å†…ã§`yield {"type": "error", "content": "..."}` å½¢å¼ã§å ±å‘Šã•ã‚Œã¾ã™ã€‚è¤‡é›‘ãªã‚¨ãƒ©ãƒ¼åˆ†é¡ã‚„é€²æ—å ±å‘Šã¯ä¸è¦ã§ã€ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºã‚’è¡Œã„ã¾ã™ã€‚

```python
class Agent:
    async def run(self, user_input: str, abort_event=None) -> AsyncGenerator[Any, None]:
        try:
            # æ­£å¸¸å‡¦ç†ï¼ˆå˜ç´”ãªcompletionãƒ«ãƒ¼ãƒ—ï¼‰
            # ... (å®Ÿè£…ã¯ä¸Šè¨˜ã®é€šã‚Š)
            
        except fastmcp.exceptions.ConnectionError as e:
            yield {"type": "error", "content": f"ãƒ„ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}"}
            
        except litellm.exceptions.APIError as e:
            yield {"type": "error", "content": f"LLM API ã‚¨ãƒ©ãƒ¼: {e}"}
            
        except AgentError as e:
            yield {"type": "error", "content": f"ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}"}
            
        except Exception as e:
            yield {"type": "error", "content": f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}"}
```

## TDDå®Ÿè£…è¨ˆç”»ï¼ˆTiny Agentï¼‰

### Phase 1: ãƒ†ã‚¹ãƒˆä½œæˆï¼ˆREDï¼‰

```python
# tests/test_agent/test_core.py
@pytest.mark.asyncio
async def test_agent_initialization():
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
    pass

@pytest.mark.asyncio
async def test_agent_context_manager():
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãƒ†ã‚¹ãƒˆ"""
    pass

@pytest.mark.asyncio
async def test_available_tools_property():
    """available_toolsãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
    pass

@pytest.mark.asyncio
async def test_run_simple_completion():
    """å˜ç´”ãªcompletionãƒ«ãƒ¼ãƒ—ãƒ†ã‚¹ãƒˆ"""
    pass

@pytest.mark.asyncio
async def test_run_with_tool_calls():
    """ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚ã‚Šã®runãƒ†ã‚¹ãƒˆ"""
    pass

@pytest.mark.asyncio
async def test_process_single_turn_streaming():
    """å˜ä¸€ã‚¿ãƒ¼ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
    pass

@pytest.mark.asyncio
async def test_execute_tool_calls():
    """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œãƒ†ã‚¹ãƒˆ"""
    pass

@pytest.mark.asyncio
async def test_problem_solved_detection():
    """å•é¡Œè§£æ±ºåˆ¤å®šãƒ†ã‚¹ãƒˆ"""
    pass

@pytest.mark.asyncio
async def test_error_handling():
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
    pass

@pytest.mark.asyncio
async def test_abort_event():
    """ä¸­æ–­ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ãƒ†ã‚¹ãƒˆ"""
    pass
```

### Phase 2: æœ€å°å®Ÿè£…ï¼ˆGREENï¼‰

1. ä¾‹å¤–ã‚¯ãƒ©ã‚¹å®Ÿè£…ï¼ˆ`AgentError`, `AgentConnectionError`ï¼‰
2. AgentåŸºæœ¬ã‚¯ãƒ©ã‚¹å®Ÿè£…
   - `__init__`, `__aenter__`, `__aexit__`
   - `available_tools` ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
   - `run` ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆåŸºæœ¬ãƒ«ãƒ¼ãƒ—ï¼‰
   - `process_single_turn_with_tools` ãƒ¡ã‚½ãƒƒãƒ‰
   - ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤

### Phase 3: ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°

1. ã‚³ãƒ¼ãƒ‰å“è³ªå‘ä¸Š
2. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–
3. å•é¡Œè§£æ±ºåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯æ”¹å–„

## ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ï¼ˆInMemory MCP Serverä½¿ç”¨ï¼‰

### ãƒ¢ãƒƒã‚¯ã¨Fixture

```python
# tests/test_agent/conftest.py
import pytest
from unittest.mock import patch, MagicMock, AsyncMock
from fastmcp import FastMCP, Client
from ygents.config.models import YgentsConfig, LLMConfig, OpenAIConfig

@pytest.fixture
def mock_agent_config():
    """ãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š"""
    return YgentsConfig(
        llm=LLMConfig(
            provider="openai",
            openai=OpenAIConfig(api_key="test-key", model="gpt-4")
        ),
        mcp_servers={}
    )

@pytest.fixture
def mock_agent_config_with_mcp():
    """MCPä»˜ããƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š"""
    return YgentsConfig(
        llm=LLMConfig(
            provider="openai",
            openai=OpenAIConfig(api_key="test-key", model="gpt-4")
        ),
        mcp_servers={
            "test_server": {}  # InMemory serverã¯è¨­å®šä¸è¦
        }
    )

@pytest.fixture
def inmemory_mcp_server():
    """InMemory MCP Serverï¼ˆFastMCPã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼‰"""
    server = FastMCP(name="TestServer")
    
    @server.tool()
    def get_weather(city: str) -> str:
        """Get weather for a city"""
        return f"Weather in {city}: Sunny, 25Â°C"
    
    @server.tool()
    def calculate(operation: str, a: float, b: float) -> float:
        """Perform basic math operations"""
        if operation == "add":
            return a + b
        elif operation == "subtract":
            return a - b
        elif operation == "multiply":
            return a * b
        elif operation == "divide":
            if b == 0:
                raise ValueError("Division by zero")
            return a / b
        else:
            raise ValueError(f"Unknown operation: {operation}")
    
    @server.resource("file://test/data")
    def get_test_data() -> str:
        """Test data resource"""
        return "This is test data"
    
    return server

@pytest.fixture
async def inmemory_mcp_client(inmemory_mcp_server):
    """InMemory MCP Clientï¼ˆå®Ÿéš›ã®FastMCP Clientï¼‰"""
    # FastMCPã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰ç›´æ¥Clientã‚’ä½œæˆ
    client = Client(inmemory_mcp_server)
    return client

@pytest.fixture
def mock_litellm_streaming():
    """LiteLLMã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ãƒ¢ãƒƒã‚¯"""
    async def mock_acompletion(*args, **kwargs):
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ¢ãƒƒã‚¯
        chunks = [
            MagicMock(choices=[MagicMock(delta=MagicMock(content="Hello", tool_calls=None))]),
            MagicMock(choices=[MagicMock(delta=MagicMock(content=" world", tool_calls=None))]),
            MagicMock(choices=[MagicMock(delta=MagicMock(content="!", tool_calls=None))]),
        ]
        for chunk in chunks:
            yield chunk
    
    with patch("litellm.acompletion", side_effect=mock_acompletion):
        yield

@pytest.fixture
def mock_litellm_with_tools():
    """ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ä»˜ãLiteLLMå¿œç­”ãƒ¢ãƒƒã‚¯"""
    async def mock_acompletion(*args, **kwargs):
        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å«ã‚€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        chunks = [
            MagicMock(choices=[MagicMock(delta=MagicMock(content="I'll check the weather", tool_calls=None))]),
            MagicMock(choices=[MagicMock(delta=MagicMock(
                content=None,
                tool_calls=[{
                    "id": "tool_call_1",
                    "function": {
                        "name": "get_weather",
                        "arguments": {"city": "Tokyo"}
                    }
                }]
            ))]),
        ]
        for chunk in chunks:
            yield chunk
    
    with patch("litellm.acompletion", side_effect=mock_acompletion):
        yield
```

### çµ±åˆãƒ†ã‚¹ãƒˆ

```python
# tests/test_agent/test_integration.py
@pytest.mark.asyncio
async def test_simple_completion_flow(mock_agent_config, mock_litellm_streaming):
    """å˜ç´”ãªcompletionãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ"""
    from ygents.agent.core import Agent
    
    async with Agent(mock_agent_config) as agent:
        results = []
        async for chunk in agent.run("Hello"):
            results.append(chunk)
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæ­£ã—ãyieldã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        content_chunks = [c for c in results if c.get("type") == "content"]
        assert len(content_chunks) > 0
        
        # å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµåˆ
        full_content = "".join(c["content"] for c in content_chunks)
        assert "Hello world!" in full_content

@pytest.mark.asyncio
async def test_tool_execution_flow(inmemory_mcp_client, mock_litellm_with_tools):
    """InMemory MCP Serverã‚’ä½¿ã£ãŸãƒ„ãƒ¼ãƒ«å®Ÿè¡Œãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ"""
    from ygents.agent.core import Agent
    from ygents.config.models import YgentsConfig, LLMConfig, OpenAIConfig
    
    # Agentã«InMemory MCP Clientã‚’æ³¨å…¥
    config = YgentsConfig(
        llm=LLMConfig(
            provider="openai",
            openai=OpenAIConfig(api_key="test-key", model="gpt-4")
        ),
        mcp_servers={"test_server": {}}
    )
    
    async with Agent(config) as agent:
        # InMemory clientã‚’ç›´æ¥æ³¨å…¥ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        agent._mcp_client = inmemory_mcp_client
        agent._mcp_client_connected = True
        
        async with inmemory_mcp_client:
            results = []
            async for chunk in agent.run("What's the weather in Tokyo?"):
                results.append(chunk)
            
            # ãƒ„ãƒ¼ãƒ«å…¥åŠ›ãƒ»çµæœãŒæ­£ã—ãyieldã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
            tool_inputs = [c for c in results if c.get("type") == "tool_input"]
            tool_results = [c for c in results if c.get("type") == "tool_result"]
            
            assert len(tool_inputs) > 0
            assert len(tool_results) > 0
            
            # å¤©æ°—ãƒ„ãƒ¼ãƒ«ãŒå‘¼ã³å‡ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
            weather_calls = [t for t in tool_inputs if t.get("tool_name") == "get_weather"]
            assert len(weather_calls) > 0
            assert weather_calls[0]["arguments"]["city"] == "Tokyo"

@pytest.mark.asyncio
async def test_inmemory_server_tools(inmemory_mcp_client):
    """InMemory MCP Serverã®ãƒ„ãƒ¼ãƒ«ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
    async with inmemory_mcp_client:
        # ãƒ„ãƒ¼ãƒ«ä¸€è¦§å–å¾—
        tools = await inmemory_mcp_client.list_tools()
        tool_names = [tool.name for tool in tools]
        
        assert "get_weather" in tool_names
        assert "calculate" in tool_names
        
        # å¤©æ°—ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
        weather_result = await inmemory_mcp_client.call_tool("get_weather", {"city": "Tokyo"})
        assert "Tokyo" in str(weather_result)
        assert "Sunny" in str(weather_result)
        
        # è¨ˆç®—ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
        calc_result = await inmemory_mcp_client.call_tool("calculate", {
            "operation": "add",
            "a": 5,
            "b": 3
        })
        assert calc_result[0].text == "8"  # TextContentã‚’æƒ³å®š
```

## é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [MCPçµ±åˆè¨­è¨ˆ](./mcp-client.md)
- [è¨­å®šç®¡ç†è¨­è¨ˆ](./config-management.md)
- [å®Ÿè£…è¨ˆç”»](../IMPLEMENTATION_PLAN.md)
- [REQUIREMENTS.md](../REQUIREMENTS.md)
- [FastMCP Documentation](https://docs.fastmcp.ai/)
- [LiteLLM Documentation](https://docs.litellm.ai/)

## ã¾ã¨ã‚

Tiny Agentã¯ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã¤ç°¡ç´ åŒ–ã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã§ã™ï¼š

1. **å˜ç´”ãªcompletionãƒ«ãƒ¼ãƒ—**: è¤‡é›‘ãªã‚¿ã‚¹ã‚¯è¨ˆç”»ãƒ»å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ä¸è¦
2. **ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ™ãƒ¼ã‚¹å¯¾è©±**: ä¼šè©±å±¥æ­´ã‚’`messages`é…åˆ—ã§ç®¡ç†
3. **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ**: LiteLLMã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°APIã‚’æ´»ç”¨
4. **ãƒ„ãƒ¼ãƒ«çµ±åˆ**: completionçµæœã«åŸºã¥ãè‡ªå‹•ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
5. **æ°¸ç¶šMCPæ¥ç¶š**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã¨åŒæœŸã—ãŸåŠ¹ç‡çš„æ¥ç¶šç®¡ç†
6. **TDDå®Ÿè£…**: ãƒ†ã‚¹ãƒˆå…ˆè¡Œã«ã‚ˆã‚‹å“è³ªç¢ºä¿

ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šã€REQUIREMENTS.mdã§å®šç¾©ã•ã‚ŒãŸã€Œå˜ç´”ãªcompletionãƒ«ãƒ¼ãƒ—ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ã€ã¨ã„ã†è¦ä»¶ã‚’æº€ãŸã™ã€ã‚·ãƒ³ãƒ—ãƒ«ã§ä¿å®ˆæ€§ã®é«˜ã„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿç¾ã—ã¾ã™ã€‚